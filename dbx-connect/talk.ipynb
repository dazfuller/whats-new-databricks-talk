{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit ('.venv')",
   "metadata": {
    "interpreter": {
     "hash": "599c3c19ecd5c8e9d227bcb8872dcdcd1f60e20ab3c4995de9e71372f5247f6e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Working with Databricks outside of the UI"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Lets query the ingested data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark here is handled by the databricks-connect library\n",
    "# \n",
    "# Once installed it's configured by calling \"databricks-connect configure\" where we provide details on how\n",
    "# to connect to our cluster. The version of databricks-connect installed must match the Databricks Runtime\n",
    "# version on the cluster\n",
    "\n",
    "spark: SparkSession = SparkSession.builder.getOrCreate()\n",
    "\n",
    "asset_id: int = int(spark.sql(\"SELECT MAX(AssetNumber) FROM silver.events\").head()[0])\n",
    "\n",
    "df: DataFrame = spark.sql(f\"SELECT * FROM silver.events WHERE AssetNumber = {asset_id}\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "source": [
    "## Now lets query the data, aggregate it, and pull it back locally as a Pandas DataFrame so that we can plot it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets read the data for the asset and bucket the data into hours with the average\n",
    "# value for the hour\n",
    "\n",
    "grouped_df: DataFrame = spark.sql((\"SELECT \"\n",
    "                                  \"    AssetNumber \"\n",
    "                                  \"    , DATE_TRUNC('HOUR', EventDate) AS EventDateHour \"\n",
    "                                  \"    , AVG(Value) AS AverageValue \"\n",
    "                                  \"FROM \"\n",
    "                                  \"    silver.events \"\n",
    "                                  \"WHERE \"\n",
    "                                  f\"    AssetNumber = {asset_id} \"\n",
    "                                  \"GROUP BY \"\n",
    "                                  \"    AssetNumber \"\n",
    "                                  \"    , EventDateHour \"\n",
    "                                  \"ORDER BY \"\n",
    "                                  \"    EventDateHour\"))\n",
    "\n",
    "grouped_pd_df: pd.DataFrame = grouped_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = \"matplotlib\"\n",
    "fig = grouped_pd_df.plot(x='EventDateHour', y='AverageValue', figsize=(20, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}